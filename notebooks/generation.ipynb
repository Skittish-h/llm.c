{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/miniconda3/envs/gpu_prog/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2061 318 262 3139 286 4881 30 "
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "encode = lambda s: enc.encode_ordinary(s)\n",
    "eot = enc._special_tokens['<|endoftext|>']\n",
    "\n",
    "tokens = encode(\"What is the capital of France?\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(tokenizer, model, prompt, max_length=64):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=max_length, \n",
    "        do_sample=False, \n",
    "        # top_p=0.95, \n",
    "        # top_k=50,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return inputs[\"input_ids\"], tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../converted_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../converted_model\", torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2061 318 262 3139 286 4881 30 \n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The\n"
     ]
    }
   ],
   "source": [
    "input_ids, generated_text = generate_text(tokenizer, model, \"Name three types of renewable energy.\")\n",
    "for input_id in input_ids[0]:\n",
    "    print(input_id.item(), end=' ')\n",
    "print()\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_prog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
